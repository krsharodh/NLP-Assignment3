{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels: 4641, Train Data: 4641\n",
      "Train Labels: 580, Train Data: 580\n",
      "Train Labels: 581\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Training Data\n",
    "trainLabels = {}\n",
    "with open(\"train.label.json\",  encoding=\"utf8\") as f:\n",
    "    trainLabels = json.load(f)    \n",
    "    \n",
    "trainingData = []\n",
    "trainingDataTweet = []\n",
    "combinedDataTrain = []\n",
    "with open(\"train.data.jsonl\", encoding=\"utf8\") as f:\n",
    "    coreIndex = 0\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        trainingData.append(row)\n",
    "        text = \"\"\n",
    "        for tweet in row:\n",
    "            text += \" \" + tweet[\"text\"]\n",
    "        combinedDataTrain.append(text[1:])\n",
    "        trainingDataTweet.append(row[0][\"text\"])\n",
    "      \n",
    "# Development Data    \n",
    "devLabels = {}\n",
    "with open(\"dev.label.json\",  encoding=\"utf8\") as f:\n",
    "    devLabels = json.load(f)    \n",
    "    \n",
    "devData = []\n",
    "devDataTweet = []\n",
    "combinedDataDev = []\n",
    "with open(\"dev.data.jsonl\", encoding=\"utf8\") as f:\n",
    "    coreIndex = 0\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        devData.append(row)\n",
    "        \n",
    "        text = \"\"\n",
    "        for tweet in row:\n",
    "            text += \" \" + tweet[\"text\"]\n",
    "        combinedDataDev.append(text[1:])\n",
    "        devDataTweet.append(row[0][\"text\"])\n",
    "\n",
    "# Test Data\n",
    "testData = []\n",
    "testDataTweet = []\n",
    "combinedDataTest = []\n",
    "with open(\"test.data.jsonl\", encoding=\"utf8\") as f:\n",
    "    coreIndex = 0\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        testData.append(row)\n",
    "        \n",
    "        text = \"\"\n",
    "        for tweet in row:\n",
    "            text += \" \" + tweet[\"text\"]\n",
    "        combinedDataTest.append(text[1:])\n",
    "        testDataTweet.append(row[0][\"text\"])\n",
    "        \n",
    "print (f\"Train Labels: {len(trainLabels)}, Train Data: {len(trainingData)}\")\n",
    "print (f\"Train Labels: {len(devLabels)}, Train Data: {len(devData)}\")\n",
    "print (f\"Train Labels: {len(testData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190056023"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[234][0][\"user\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = list(trainLabels.values())\n",
    "y_dev = list(devLabels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(combinedDataTrain)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(combinedDataTrain, mode=\"count\") #BOW representation\n",
    "x_dev = tokenizer.texts_to_matrix(combinedDataDev, mode=\"count\") #BOW representation\n",
    "x_test = tokenizer.texts_to_matrix(combinedDataTest, mode=\"count\") #BOW representation\n",
    "\n",
    "vocab_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenise the input into word sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(trainingDataTweet)\n",
    "\n",
    "xseq_train = tokenizer.texts_to_sequences(combinedDataTrain)\n",
    "xseq_dev = tokenizer.texts_to_sequences(combinedDataDev)\n",
    "xseq_test = tokenizer.texts_to_sequences(combinedDataTest)\n",
    "\n",
    "len(xseq_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500\n",
    "xseq_train = pad_sequences(xseq_train, padding='post', maxlen=maxlen)\n",
    "xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=maxlen)\n",
    "xseq_test = pad_sequences(xseq_test, padding='post', maxlen=maxlen)\n",
    "print(xseq_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8379310344827586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_dev, y_dev)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(['non-rumour', 'rumour'])\n",
    "y_train_binary = le.transform(y_train)\n",
    "y_dev_binary = le.transform(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 10\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model3 = Sequential(name=\"lstm\")\n",
    "model3.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model3.add(LSTM(10))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(xseq_train, y_train_binary, epochs=20, verbose=True, validation_data=(xseq_dev, y_dev_binary), batch_size=10)\n",
    "\n",
    "loss, accuracy = model3.evaluate(xseq_dev, y_dev_binary, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_LSTM = model3.predict(xseq_test)\n",
    "y_test_LSTM = list(map(lambda label : int(round(label[0])), y_test_LSTM))\n",
    "y_predict_LSTM = le.inverse_transform(y_test_LSTM)\n",
    "\n",
    "\n",
    "data = {}\n",
    "\n",
    "for i in range(len(testData)):\n",
    "    data[testData[i][0]['id']] = y_predict_LSTM[i]\n",
    "\n",
    "with open('test-output.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
